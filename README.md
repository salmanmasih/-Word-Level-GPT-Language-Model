# ğŸ§  Word-Level GPT Language Model

This project implements a simplified GPT-style autoregressive language model trained on the **Tiny Shakespeare** dataset using **word-level tokenization**.

---

## ğŸš€ Features

- âœ… Word-level vocabulary creation from raw text  
- âœ… Custom encoding/decoding functions for word â†” index  
- âœ… PyTorch-based GPT model implementation from scratch  
- âœ… Positional embeddings + causal self-attention  
- âœ… Transformer block with multi-head attention and feed-forward layers  
- âœ… Text generation using sampling from softmax output  
- âœ… Training and validation loss monitoring  
- âœ… Support for CPU and GPU (CUDA)

---

## ğŸ“‚ Project Structure

```
word-gpt/
â”œâ”€â”€ gpt_model.py                # GPT model architecture (attention, transformer blocks)
â”œâ”€â”€ train_gpt.py                # Training loop and generation logic
â”œâ”€â”€ tokenizer_utils.py          # Word-level vocabulary, encode/decode utilities
â”œâ”€â”€ README.md                   # Project overview and setup
```

---

## ğŸ“Š Dataset

- **Source**: [Tiny Shakespeare dataset](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)  
- **Tokenization**: Word-based (via `.split()`)

---

## âš™ï¸ Hyperparameters

- `block_size`: 32 (context window size)  
- `batch_size`: 16  
- `embed_dim`: 128  
- `n_layers`: 4  
- `num_heads`: 4  
- `ffw_hidden_dim`: 256  
- `learning_rate`: 3e-4  
- `max_steps`: 1000

---

## ğŸ”§ How to Run

```bash
# Install PyTorch and dependencies
pip install torch

# Run training script
python train_gpt.py

# Sample generation
python generate_text.py
```

---

## ğŸ§ª Sample Output

```text
Prompt: House  
Generated: House of the day of the world to be the same to be the most of the man and
```

---

## ğŸ“ˆ Future Improvements

- Add temperature/top-k sampling for more diverse generation  
- Switch to subword-level encoding (BPE)  
- Add CLI or notebook interface for prompt-based generation  

---

## â¤ï¸ Acknowledgments

- Inspired by [Andrej Karpathy's minGPT](https://github.com/karpathy/minGPT)  
- Built from scratch using PyTorch

---

## ğŸ“œ License

This project is open-source and licensed under the MIT License.
